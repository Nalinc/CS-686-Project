\def\year{2017}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper] {article} %DO NOT CHANGE THIS
\usepackage{aaai17}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{multirow}
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{tcolorbox}
\usepackage{caption}

\usepackage{csquotes} %Added from CSCW Paper
\usepackage{comment}
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
  \pdfinfo{
/Title			
/Author (AAAI Press Staff)}
\setcounter{secnumdepth}{0}  
\begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Designing a neural conversational agent: nBot}
\author{Nalin Chhibber\\ Student ID: 20715659, MMath CS(HCI)}
\maketitle
\begin{abstract}
The primary aim of this paper is to describe popular techniques that researchers have been using to build conversational agents and cover a sizable body of literature related to the area. Additionally, this paper replicates the results by \cite{lowe2015ubuntu} for Unstructured Multi-Turn Dialogue Systems. Based on the findings, the paper also introduces a new conversational agent: nBot. nBot is trained on a limited dataset of manually constructed sentences and built with a combination of both retrieval-based and generative techniques. It uses Long Short Term Memory to remember the context of conversation.
\end{abstract}

\section{Introduction}
%\begin{figure}[ht]
%\centering
 % \includegraphics[width=0.8\columnwidth]{figures/sample.jpg}
%\caption{Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim \textit{veniam}, quis nostrud exercitation ullamco laboris }
%\label{fig:workflow}
%\end{figure}
As we are gradually incorporating technology in our lives, we are also trying to simplify the interaction technique by getting rid of unused mediums from the space of complicated interfaces. Taking the example of a simple phone, we started using them with rotary dials, then replaced those with numeric buttons and moved on to using touch screens. Voice user interaction is the next logical step and we have already started seeing a glimpse of it in Siri and Google Assistant. While most of the existing modes of human-computer interaction (touch/type/click) require users to adapt with the interface(screen/keyboard/mouse), voice user interfaces on the other hand, comes natural to humans and hence can be used as one of the most promising medium to engage them in a productive interaction. There has been a lot of optimism in the thought that near future will witness a rapid growth in human-computer-interaction using voice. This has not only led to an increased demand of conversational agents but also shown an increase in various chatbot development frameworks. Brands are increasingly using chatbots to engage their customers. Within just a couple years, we have seen a different evolution in the design of conversational agents from chatbots in Facebook Messenger, to Siri in iphones, to Microsoft's Cortana, Google Home and Amazon Alexa. While we cannot predict when Watson or Siri will start working like Jarvis, there is a still a lot that we can do with the normal scripted conversational agents.\\\\
This paper has implemented a simple conversational agent, nBot using a combination of retrieval-based and generative models. It uses a recurrent neural network with 4 hidden layers of LSTM cells to understand the context and respond to user utterances. It has been trained on a limited dataset of manually constructed conversations over 5000 epochs.\\\\
Rest of the paper describes taxonomy of models and related work around the design of conversational bots.  It then details the implementation of nBot, followed by discussions on future work.

\section{Taxonomy of models}
%The term 'chatbot' covers a number of categories including stand-alone applications, AI tools, bot developer frameworks, messaging services, bot discovery, and analytics. This paper is mainly concerned with version that defines them as conversational agents.
\subsection{Retrieval based vs Generative vs Pattern based}
Challenge of building a conversational bot can be addressed using a retrival-based, generative or pattern-based model. Retrieval-based models are those which have a repository of pre-defined responses to answer a user utterance. Alicebot and Cleverbot are examples of such types. On the other hand, generative models are those which can generate responses they have never seen before. Microsoft's Tay bot is an example of generative model. Generative models are usually based on machine translation techniques but instead of translating from one language to another, they translate from input uttrance to output response. Both approaches have some obvious pros and cons. Retrieval based techniques dont make gramatical mistakes and are easier to train due to repository of handcrafted rules. However they may be unable to handle unseen cases for which no appropriate response exists. For the same reason these models cant refer back to contextual entity information. Generative models overcome this limitation and can refer back to entities in the input. However these models are harder to train, likely to make gramatical mistakes, produce inconsitant or irrelevant responses and require huge amounts of training data.  %For these reasons most of the production system these days are either retrieval based or a combination of both.
\\\\
Another technique is to use pattern-based heuristics for selecting a response. In these techniques, when the agent receives a message, it goes through all the patterns until it finds a pattern which matches user utterance. If the match is found, the chatbot uses the corresponding template to generate a response. The problem with pattern-based heuristics is that patterns should be programmed manually which is not easy, especially if the agent has to correctly distinguish hundreds of intents. Users can express the same intent in many ways or use similar words in different contexts. Machine learning can help us train an intent classificaltion algorithm to pick patterns in the data. Most of the existing chatbot frameworks(wit.ai, dialogflow, Microsoft LUIS) are based on this technique.

\subsection{Open-domain vs Close-domain}
In open-domain, conversations dont have a well defined goal or intention and can jump to any topic from dating to space travel to politics. Conversations on social media sites like Reddit and Twitter are typically open domain. In closed domain, space of possible input utterance is limited to a specific topic(education, casual conversation, technical support). It is realtively easy to model a conversational agent in close domain. Figure 1 illustrates relative difficulity in designing chatbots with different techniques.
\begin{figure}[ht]
	\centering
	\includegraphics[width=200px]{figures/chatbots.png}
	\caption{Chatbot Conversation Framework\cite{chatbot:framework}}
	\label{fig:chatbotframework}
\end{figure}
\subsection{Online vs Batch Learning}
Batch learning generates the best predictor by learning on the entire training data set at once. Online learning on the other hand uses new responses to update the best predictor for future responses at each step. Online learning can dynamically adapt to new patterns in the conversation as it is generated as a function of time. One problem with online learning is to control unwanted or potentially harmful information. One significant instance is when Microsoft's Twitter chatbot: Tay, started posting a deluge of incredibly racist messages in response to questions. Tay was designed to learn from conversations and get progressively "smarter" but online troublemakes and trolls persuaded it to blithely use racial slurs and even outright call for genocide \cite{microsoft:tay}. Therefore online learning should be assisted by other techniques to prevent them from going off the rails.

\subsection{End-to-end vs Distributed Learning}
Distributed learning systems require multiple stages of processing and follow a set pipeline for solving tasks at hand from feature extraction to learning the desired result. End-to-End learning on the other hand omits any hand-crafted intermediatery algorithms and directly learns the solution to a given problem from sampled dataset. This could involve concatenation of different neural networks(CNNs, RNNs) which are trained simultaneously. The idea is to let the network go from"raw-est" possible data to "final-most" output.

\section{Related work}
Typical dialogue systems consist of a Speech Recognizer, Language Interpreter, State Tracker, Response Generator, Natural Language Generator, and Speech Synthesizer. They can further be distinguished as goal driven systems, or non-goal driven systems. Initial work on goal driven dialogue systems primarily used rule-based systems with the distinction that machine learning techniques were used to classify the intention (or need) of the user, as well as to bridge the gap between text and speech\cite{serban2015survey}. Research in this area started to take off during the mid 90s, when researchers began to formulate dialogue as a sequential decision making problem based on Markov decision processes. \cite{young2013pomdp} \cite{singh2000reinforcement} \cite{pieraccini2009we}. With these systems, each of the components were trained separately to do their own task and the chatbot used to collectively use results from each to give the response. Later, with the improvements in deep learning algorithms, conversational models started using an end-to-end approach and use Neural Networks to predict the output of given input utterance. Though simple Feedforward Neural Networks with one hideen layer are regarded as universal function approximators, they are not well suited to remember timeseries data and thus cannot refer the information from past conversation. A Recurrent Neural Network (RNN) solves this problem by looping back part of the output back into the network, allowing information to persist. RNNs reads the input sentence, one token at a time, and predicts the output sequence, one token at a time. However, even RNNs are not too good in remembering long term dependencies and tend to forget the context if the gap between the relevant information and the point where it is needed becomes very large. Long Short Term Memory networks or LSTM are special types of RNNs and they are capable of learning long-term dependencies \cite{Hochreiter:1997:LSM:1246443.1246450}. Besides the neural network itself, another thing that determines the performance of a conversational agent is the language model which gives the probability distribution over a sequences of words. This language model is based on the input to the neural network which involves elements of Natural Language Processing.\\\\
Traditional NLP models typically use word as an atomic unit. However, words are discrete in nature and it seems nonsensical to feed word indexes to Neural Networks. A typical approach is to map a discrete word to a dense, low-dimensional, real-valued vector, called an embedding. A popular word embedding model is word2vec which uses a distributed representation of word to capture meaningful syntactic and semantic regularities \cite{mikolov2013efficient} \cite{mikolov2013distributed} \cite{mikolov2013linguistic}. In \cite{le2014distributed}, authors  extends the same idea to also compute distributed representations for sentences, paragraphs, and even entire documents. These vector representations are used in building the neural language model and help RNNs learn the context over time, given a word. We can choose how large these vectors should be(nBot uses vectors of size 300 dimensions). Both the embedded context and response are fed into the same Recurrent Neural Network word-by-word. The RNN generates a vector representation that, loosely speaking, captures the "meaning" of the context and response, and overtime learns to output the correct message. Word Embedding is typically done in the first layer of the Neural Network : Embedding layer, that maps a word (index to word in vocabulary) from vocabulary to a dense vector of given size. Alternatively, it is also possible to use a pre-trained word embeddings model with existing structure of Recurrent Neural Network.
\\\\
Some other deep Learning architectures like Sequence to Sequence\cite{sutskever2014sequence} are uniquely suited for generating text and researchers are hoping to make rapid progress in this area. Though nBot does not use sequence-to-sequence framework, it is important to mention its relevance in this paper given its performance and acceptability amongst AI researchers. Sequence-to-sequence framework consists of two RNNs (Recurrent Neural Network) : an Encoder and a Decoder. The encoder takes a sequence(sentence) as input and processes one symbol(word) at each timestep. Its objective is to convert a sequence of symbols into a fixed size feature vector that encodes only the important information in the sequence while losing the unnecessary information. \cite{vinyals2015neural} have used sequence-to-sequence framework to build a Neural Conversational Model(NCM). They trained their model on both a domain specific IT helpdesk dataset, and  a noisy open-domain movie transcript dataset. Sample conversations generated by the interaction between human actor and the NCM reports lower perplexity as compared to n-grams model and even outperformed CleverBot in a subjective test involving human evaluators to grade the two systems. However, as with any generative model, primary limitation of NCM is short and at times inconsistent responses. Moreover, the objective function of sequence-to-sequence framework is not designed to capture the objective of conversational models(though the underlying architecture can be leveraged for machine translation, and question answering applications).
\\\\
Rest of this section is focused on discussing "The Ubuntu Dialogue Corpus" paper and replicating their results. Ubuntu Dialog Corpus is a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words \cite{lowe2015ubuntu}. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. To justify the usefulness of proposed dataset for research in neural architectures for dialogue managers, authors provided a performance benchmarks for two neural learning algorithms(RNNs and LSTMs), as well as one naive baseline(TF-IDF). Prior to applying each method, they performed standard pre-processing of the data by tokenizing, stemmimg, and lemmatizing the utterances using NLTK. They also used generic tags for various word categories(like names, locations, organizations, urls). Additionally, authors set aside 2\% of the Ubuntu Dialogue Corpus conversations (randomly selected) to form a test set that can be used for evaluation of response selection algorithms. Compared to the rest of the corpus, this test set has been further processed to extract a pair of (context, response, flag) triples from each dialogue. The \textit{flag} is a Boolean variable indicating whether or not the response was the actual next utterance after the given context, \textit{response} is a target (output) utterance which we aim to correctly identify, and finally \textit{context} consists of the sequence of utterances appearing in dialogue prior to the response. In order to evaluate each of the three methods, authors used Recall@k metric. Here the agent is asked to select the k most likely responses, and test example is marked correct if the true response is among these k candidates. So a larger k means that the task becomes easier. In a 1-in-10 utterance classification, if we set k=10 we get a recall of 100\% because we only have 10 responses to pick from. If we set k=1 the model has only one chance to pick the right response.

\subsubsection{TF-IDF:}
Term frequency-inverse document frequency is a statistic that intends to capture how important a given word is to some document, which in our case is the context \cite{ramos2003using}. It is a technique often used in document classification and information retrieval. The 'term-frequency' term is simply a count of the number of times a word appears in a given context, while the ‘inverse document frequency’ term puts a penalty on how often the word appears elsewhere in the corpus. The final score is calculated as the product of these two terms, and has the form: 
$$
tfidf(w,d,D) = f(w,d)* \log\frac{N}{|{d \in D:w \in d}|} 
$$
where f(w, d) indicates the number of times word w appeared in context d, N is the total number of dialogues, and the denominator represents the number of dialogues in which the word w appears. For classification, the TF-IDF vectors are first calculated for the context and each of the candidate responses. Given a set of candidate response vectors, the one with the highest cosine similarity to the context vector is selected as the output. For Recall@k, the top k responses are returned. 

\subsubsection{RNNs:}
Recurrent neural networks are a variant of neural networks that allows for time-delayed directed cycles between units\cite{medsker1999recurrent}. This leads to the formation of an internal state of the network, $h_t$ , which allows it to model time-dependent data. The internal state is updated at each time step as some function of the observed variables $x_t$ , and the hidden state at the previous time step $h_{t-1}$. $W_x$ and $W_h$ are matrices associated with the input and hidden state.
$$
h_t = f(W_h W_{t-1} + W_x x_t)
$$

\subsubsection{LSTM:} 
In addition to the RNN model, original authors considered the same architecture but changed hidden units to long-short term memory (LSTM) units\cite{hochreiter1997long}. LSTMs were introduced in order to model longerterm dependencies. This is accomplished using a series of gates that determine whether a new input should be remembered, forgotten (and the old value retained), or used as output. The error signal can now be fed back indefinitely into the gates of the LSTM unit. This helps overcome the vanishing and exploding gradient problems in standard RNNs, where the error gradients would otherwise decrease or increase at an exponential rate. In training, we used 1 hidden layer with 200 neurons. 
%The hyper-parameter configuration (including number of neurons) was optimized independently for RNNs and LSTMs using a validation set extracted from the training data.
\\\\
Following tables illustrate the perfomance for above three models based on Recall@k metric from original paper and replicated results wrt TF-IDF and LSTM. For comparison, completely random predictor got a score of 9.3\%, 19.4\% and 49.2\% for Recall@1, Recall@2 and Recall@5 respectively.
\begin{center}
	\begin{tabular}{ |c|c|c|c| } 
		\hline
		\textbf{Method} & \textbf{TF-IDF} & \textbf{RNN} & \textbf{LSTM} \\ \hline
		Recall@1 & 41.0\%  & 40.3\% & 60.4\% \\ \hline
		Recall@2 & 54.5\% & 54.7\% & 74.5\% \\ \hline
		Recall@5 & 70.8\% & 81.9\% & 92.6\% \\ \hline
	\end{tabular}
	\captionof{table}{Results for the three algorithms using various recall measures}
\end{center}
\begin{center}
	\begin{tabular}{ |c|c|c| } 
		\hline
		\textbf{Method} & \textbf{TF-IDF} & \textbf{LSTM} \\ \hline
		Recall@1 & 49.5\% & 49.84\% \\ \hline
		Recall@2 & 59.7\% & 68.74\% \\ \hline
		Recall@5 & 76.6\% & 91.44\% \\ \hline
	\end{tabular}
	\captionof{table}{Replicated results for the three algorithms using various recall measures after 2440 steps}
\end{center}
The replicated model gives following response after training. Spelling mistakes are due to the quality of the corpus.
\begin{tcolorbox}
     \textbf{[Context]} be i abl to use dhcpd toissu dhcp to all subnet except the one it be in ? \_\_eou\_\_ \_\_eot\_\_ great question ! sound implaus but i ll check \_\_eou\_\_ \_\_eot\_\_ thank . i ve be read the configur guid and ca n't find if i can or not . i could off cours tri . \_\_eou\_\_ \_\_eot\_\_ the way i read it , for a multi-hom server , it ll onli server sub-net for which there be a subnet { } claus in the config \_\_eou\_\_ \_\_eot\_\_ \\
     \textbf{[Answer]} so i would put a shared-network group with the subnet i want in there , exclud the one it be attach to ? \_\_eou\_\_
\end{tcolorbox}
The AI and Deep learning blog by \cite{DENNYBRITZ} helped a lot while replicating the results of original authors. Inside the recurrent neural network, input context(c) is multiplied with a matrix M to "predict" a response r'. Input context(c) is a 300-dimensional vector, M is a 300*300 dimensional matrix, and the result is another 300-dimensional vector, which can be interpreted as a generated response. The matrix M is learned during training. The model then measures similarity of the predicted response r' and the actual response r by taking the dot product of these two vectors. A large dot product means the vectors are similar and that the response should receive a high score. I then applied a sigmoid function to convert that score into a probability.

\section{Implementation}
As mentioned earlier, nBot uses a pre-trained word embeddings model from Associated Press News for training. The conversational model itself has been trained on keras using tensorflow as a backend. It uses a recurrent neural network consisting of 4 hidden layers with LSTM cells each having an output dimension of 300. In order to train the neural network, it is important that the input is compatible with the hidden layers. For this, we did a preprocessing on input data where we created two lists for input queries and output responses. One catch with using neural networks is that they can only be applied to problems where inputs and outputs can sensibly be encoded with vectors of fixed dimensionality. Therefore, every sentence was clipped to 15 words and each word was represented as a word vector of length 300. For situations where a sentence was shorter than 15 words, we padded the input vector with numpy vector of size 300 consisting of 1's. For every word in the corpus, we tried to find a vector for it from the pretrained word2vec model which effectively modeled a sentence to be a vector of word-vectors. This vector of word-vectors is eventually fed to the recurent neural network. Weight initialization as well as the choice of optimizer are critical for training the RNNs. We have used "ADAM optimizer" with "Cosine Proximity" as the loss/cost function. ADAM stands for "Adaptive Momentum Estimation" and is an improvement over gradient descent. In adam, we calculate the first moment, the mean and the 2nd moment, the un-centered variance of the gradients respectively. Then we use those values to update the parameters just like in AdaDelta. This technique has been proven to converge quickly than other variations of gradient descent\cite{kingma2014adam}. Further, the model has used "Glorot Normal" initializer to set the initial random weights of Keras layers. Before training the model on LSTM cells, we reserved 20\% of total input data to be used as a validation set and test the model accuracy. This train-test split was achieved using sklearn.model\_selection.
\\\\
Though nBot is a functional conversational agent, it is still pretty naive in handling existing utterances and gives glibberish response on receiving unknown/new inputs. This result has been achieved on an accuracy of mere 31.67\% on training data with a loss of -0.8417. The negative loss indicates cosine proximity and indicates that model is trying to make proximity as high as possible by minimizing a negative scalar. A matter of concern is low accuracy(5.19\%) on validation data, but it is understandable as the manually constructed corpus is not feature rich. Following section illustrates a sample conversation between a human and nBot after training for 5000 epochs. These alien words are due to the presence of padding elements in input.
\\
	\begin{tcolorbox}
	\textbf{User:}hi \\
	\textbf{nBot:}how are you doing ? kleiser kleiser karluah karluah karluah kleiser kleiser kleiser kleiser kleiser \\
	\textbf{User:}Did I ever live?\\
	\textbf{nBot:}it depends how you define life ballets kleiser karluah karluah karluah kleiser kleiser kleiser kleiser\\
	\textbf{User:} Who are you?\\
	\textbf{nBot:} what ? who is but a form following the function of what kleiser kleiser karluah\\
	\end{tcolorbox}
Figure-2 and Figure-3 illustrate the change of accuracy and loss of model with respect to each iteration over the period of 5000 epochs. Low accuracy and high loss in test data indicates a presense of overfitting. This could be true and due to the less-diverse nature of corpus on which nBot is trained.

\begin{figure}[ht]
	\centering
	\includegraphics[width=200px]{figures/accuracy.png}
	\caption{Training vs testing accuracy in nBot over 5000 epochs}
	\label{fig:nbotaccuracy}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=200px]{figures/loss.png}
	\caption{Training vs testing loss in nBot over 5000 epochs}
	\label{fig:nbotaccuracy}
\end{figure}
\section{Discussion}
Despite average performance in simulating a human-like conversation, nBot still manages to give decent response for known inputs. Though model accuracy can be improved with further training and trying various combinations of activation and loss functions, there are still some major challenges involved in designing a full conversational agent that really improves human computer interaction in voice-only fashion. Following are some of those challenges:

\subsection{Context Awareness}
In real conversations, people don't start thinking from scratch every second to come come up with a new response. They build the conversation based on relevant concepts they might have discussed in past. Therefore, to produce sensible responses systems may need to incorporate both linguistic context and physical context. The most common approach is to embed the conversation into a vector, but doing that with long conversations is challenging in simple recurrent neural networks. Experiments by \cite{serban2016building} and \cite{yao2015attention} both go into that direction. Besides maintaining the context of topic, a smart conversational agent should also incorporate other kinds of contextual data such as date/time, location and information about a user.

\subsection{Consistent Personality}
When generating responses the agent should ideally produce consistent answers to semantically identical inputs. For example, the response to inputs like "How old are you?" and "What is your age?" should be same. This might sound simple, but incorporating such fixed knowledge or "personality" into models is very much a research problem. Since most of the models are trained on data from multiple different users, they tend to give inconsistent responses to the same user utterance. Works like \cite{li2016persona} has proven to be first steps into the direction of explicitly modeling a personality in a conversational bot.
\subsection{Evaluation of Models}
The ideal way to evaluate a conversational agent is to measure whether or not it is fulfilling its task.However such labels are expensive to obtain as there is no well-defined goal. Common metrics such as BLEU(Bilingual Evaluation Understudy) that are based on text matching and used for Machine Translation aren’t well suited because sensible responses can contain completely different words or phrases. In \cite{liu2016not}, authors have concluded that none of the commonly used metrics really correlate with human judgment.
\subsection{Intention and Diversity}
One of the prime issues with most fo the retrieval-based and generative models is lack of diversity in responses. Google's smart reply is a good example of this whose earlier version used to respond with "I love you" to almost anything. This is partly due to how these systems are trained, both in terms of data and in terms of actual training objective/algorithm.  Some researchers have tried to artificially promote diversity through various objective functions \cite{li2015diversity}. Humans conversations are typically specific to the input and carry an intention. Since generative models aren’t trained to have specific intentions they lack this kind of diversity.
\subsection{Choice of corpus}
A corpus is a principled collection of texts, written or spoken, which is stored on a computer. It must represent something and its merits are often judged based on how representative it is. There are many corpora available and some can be bought, some are free and some are not publicly available. There is no one corpus to suit all purposes and choice of corpora is really important while training a conversational agent for a specific purpose \cite{o2007corpus}. For an effective research in dialog systems, corpus should have following qualities:
\begin{itemize}
	\item Two-way (or dyadic) conversation, as opposed to multi-participant chat.
	\item Large number of conversations.
	\item Many conversations with several turns(more than 3).
	\item Task-specific domain, as opposed to chatbot systems.
\end{itemize}
Most of these requirements are satisfied by the Ubuntu Dialogue Corpus introducted by \cite{lowe2015ubuntu}. However, the dataset is more suited for a technical-support domain and not specifically made to train a practical conversational agent. Some popular corpus that have been used to train conversational bots in past are as follows:
\begin{itemize}
	\item NPS Chat Corpus: Consists of 10,567 posts in release 1.0. It is a part of python NLTK package.
	\item NUS Corpus: Collection of SMS messages compiled by NLP group at National University of Singapore.
	\item Ubuntu Dialogue Corpus: Consists of 1,000,000 examples based on chat logs from the Ubuntu channels on a public IRC network.
	\item Cornell Movie-Dialogs Corpus:  Contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts: 220,579 conversational exchanges between 10,292 pairs of movie characters.
	\item Microsoft Research Social Media Conversation Corpus: Collection of 12,696 Tweet Ids representing 4,232 three-step conversational snippets extracted from Twitter logs.
\end{itemize}
This paper replicate the results of \cite{lowe2015ubuntu} for Unstructured Multi-Turn Dialogue Systems which uses Ubuntu Dialog Corpus. UDC is a big corpus and training a conversational agent on so much data with limited computing resources (without gpu) is chalenging. For these reasons, nBot is trained on a manual corpus based on general conversational dialogs. to get better results after training nBot on a bigger and feature rich corpus in future.

\section{Limitations}
Current model has naively assumed that a speech-to-text recogniser can be bolted in front of the system in order to build a voice-driven assistant. However, the distribution of utterances changes dramatically according to the nature of the interaction. Besides, the current accuracy of nBot is not quite impressive due to two primary reasons. First, it has been trained on a small dataset of 86 statements divided into 12 blocks to simulate context. Second, nBot has not been trained for sufficiently large time. Present version has been trained for 5000 epochs which itself took a couple hours. Another limitation of nBot is the restriction of having no more than 15 words in input sentences.

\section{Future Work}
Author has submitted an ORE application to carry out field studies in early 2018. Though the study is focused on voice-only interaction modes in HCI and not directly related to artificial intelligence, it might use nBot after improving accuracy to check effectiveness of certain conversational styles in voice-only interactions. Since the current implementation is still in textual mode that runs without server, first step in moving forward would be to deploy nBot on a cloud service and expose its conversational end in the form of an API. Later, it will be linked with a TTS-STT module(text-to-speech/speech-to-text) or integarted with any of the existing chatbot frameworks(dialogflow, ask, watson services) to realize actual voice-only interaction. In this entire process, the primary challenge will be the AI problem of finding right response from an infinite search-space to achieve an effective conversation.

\bibliographystyle{aaai}
\bibliography{ta}
\newpage
\section{Appendix}
\begin{itemize}
	\item Project repository: https://github.com/Nalinc/CS-686-Project/
	\item Code for nBot: https://github.com/Nalinc/CS-686-Project/tree/master/src/nBot
	\item Corpus used in training: https://github.com/Nalinc/CS-686-Project/blob/master/src/conversation.json
	\item Code for replicated results of Ubuntu Dialog Corpus paper \cite{lowe2015ubuntu} : https://github.com/Nalinc/CS-686-Project/tree/master/src/lowe2015ubuntu
\end{itemize}
\end{document}

